<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
		<link rel="stylesheet" href="/ProgrammingTutorial/static/main.css"/>
		<link rel="stylesheet" href="/ProgrammingTutorial/static/highlight.css"/>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<title>A History of Programming | An Unconventional Programming Tutorial</title>
	</head>
	<body>
		<div class="content-container">
			<h1 id="a-history-of-programming">A History of Programming</h1>
<p>There are lot of terms we might choose to define what we consider a computer by the standards of today and as such it is very difficult to point definitively at an early machine and say without question that it was the first. If we define computers as machines that are electronic and digital but most of all programmable — the process it performs can be changed — then we might say the first computer was the Colossus. Designed by Post Office telephone engineer Tommy Flowers starting in 1943, this machine was built to assist British intelligence at Bletchley Park with breaking the Lorenz cipher used by German High Command. The machine was successful in its task and eleven more were eventually built but, since it remained secret until the 1970s, its influence on the history of computing is limited. Our interest in it here, though, is in how it was programmed: its operation was defined by a series of switches that physically rewired the machine to perform the desired task. Its design was, in effect, not unlike that of a telephone switchboard; it had, after all, been designed by a telephone engineer.</p>
<figure>
<img src="/ProgrammingTutorial/static/images/Colossus.jpg" alt="Dorothy du Boisson (left) and Elsie Booker (right) of the Women’s Royal Naval Service operate a Colossus Mark II in 1943. [Photo Credit: National Archives via Wikimedia Commons]" /><figcaption>Dorothy du Boisson (left) and Elsie Booker (right) of the Women’s Royal Naval Service operate a Colossus Mark II in 1943. [Photo Credit: National Archives via Wikimedia Commons]</figcaption>
</figure>
<p>In these early days of computing, this was exactly the form that programming took: large panels of switches, dials and so forth were used to wire the machine to perform a desired task. This was the case with the American ENIAC (Electronic Numerical Integrator and Computer, built 1945), the first to be general purpose: unlike the Colossus that could only be programmed for particular tasks relevant to the cryptanalysis of the Lorenz, the ENIAC could theoretically be programmed to perform any numerical process. It is its six original programmers, therefore, who were the first to program a computer in any modern sense: Kay McNulty, Betty Jennings, Betty Snyder, Marlyn Meltzer, Fran Bilas and Ruth Lichterman. They were tasked with taking a description of a process and wiring this into ENIAC’s various panels. While passing mostly uncredited at the time, many of the key concepts of programming (such as subroutines, which we’ll see later) were first created by these six women for the ENIAC.</p>
<p>Back then, programming was a process of configuring a very general machine to perform a specific process. That is, perhaps, what it still is today. But we have long since left those switchboard-like banks of controls behind, we now think of programming as writing some sort of “code” that instructs the computer. What is this “code” and where did it come from? To understand this, we must look to the notion of the stored-program computer.</p>
<p>In the 1930s, the British mathematician and later cryptanalyst and computer scientist Alan Turing was considering one of mathematics’ most fundamental questions: the <em>Entscheidungsproblem</em>. The problem asks for a mathematical process (an algorithm) that determines if a given statement can be proved from a set of mathematical axioms. Turing proved that this was impossible to solve in 1936 and, although he was not the first (Alonzo Church had produced a proof earlier in the year), in doing so he introduced a concept that would become the groundwork for theoretical computer science: the Turing machine. The Turing machine provided a simple mathematical model of computing based on a hypothetical machine manipulating symbols on a tape according to a table of rules. Despite its simplicity, Turing proved that this model could be used for <em>any</em> mathematical process i.e. any algorithm or, as it would come to be seen, any computer program. From this, Turing produced an astonishing idea: a Turing machine, called the Universal Turing Machine, could be created that would simulate any other Turing machine described on its tape. It was possible, in a fundamental mathematical way, to create a machine that could perform any algorithm without needing any physical modification. There could be a computer where the program was part of the input, not part of the machine. It is hard to underestimate the significance of this idea. <span class="sidenote">It is important to remember that Turing machines are only a mathematical model. Building a physical Turing machine, though perhaps interesting, does not result in a useful computer. This is why it is such a long time from this theoretical conception to the first real computing machines. </span></p>
<p>This idea, which came to be known as the stored-program computer, seemed a logical next step from the physically programmed computers like ENIAC. It promised to change programming from an activity that required days of rewiring even for the simplest of tasks to something that could be done in just a few hours. It promised to change computing from a niche tool mostly used for only a few military research questions (the main use of ENIAC) to something accessible for a whole range of academic and industrial applications. It was not long after ENIAC was publicly announced in 1946 that the first stored-program computers emerged. The very first, Manchester University’s Small-Scale Experimental Machine (most commonly known as the Manchester Baby), ran its first program in 1948 demonstrating for the first time all of the key ideas behind modern computing. It was closely followed by Cambridge University’s Electronic Delay Storage Automatic Calculator (EDSAC) in 1949, a more sophisticated machine compared to the experimental Manchester Baby, and then by ENIAC’s successor the EDVAC (Electronic Discrete Variable Automatic Computer) in the same year. The designs of these machines formed the basis of the first commercially available computers: the British Ferranti Mark 1 in 1949 and the American UNIVAC I in 1951.</p>
<p>But our interest in the machines is only incidental, our real concern is about what they meant for programming. The stored-program computer meant, of course, that the computer program was no longer a physical aspect of wiring but some form of code inputted to the computer. But what did this code look like? Each computer could perform a set of simple operations (things like adding two numbers or moving information from one location in its memory to another) and a sequence of these operations constituted a program. Each operation was associated with a number (known as an opcode) and to create a program a programmer would write out a sequence of instructions for the computer: each consisting of one of these opcodes and then the numbers or memory locations (also represented by numbers) that the operation was to be applied to. It was purely numeric instructions like this, typically written into punched cards or tapes, that commanded these early stored-program machines. Amazingly, however, today’s computer processors still work much the same as they did back then just five years after the first real computer: their code is still given as these numeric instructions. Of course, it is a rare programmer who still writes this numeric “machine code” out today but, nevertheless, this is still the final form that every program is converted to before or as it runs.</p>
<p>With the advent of the stored-program computer, the stage was set for the next step: replacing these awkward numeric codes with new notations, ones built around usability for humans, not around the designs of machines. It was time for the birth of the programming language.</p>
<p>The first step away from numeric machine code was made almost as soon as it entered the scene. Cambridge University’s EDSAC, which we’ve already seen as perhaps the second ever stored-program computer, included, hard-wired at the start of its memory, some instructions known as the “initial orders”. These provided an assembler, a simple but important development. Instead of having to use a purely numeric representation of instructions, the EDSAC’s programmers could use written textual mnemonics in place of the opcodes and were saved the tedium of calculating memory locations. They could then punch this on to a paper tape, put it in the computer and the assembler would convert it into machine code before it ran. This made instructions easier to read and write and before too long there were assemblers for effectively every one of the new stored-program computers. Programmers today still use assemblers when they need to write programs at the instruction level. It is important, though, not to overstate the significance of these “assembly languages”: programming was still all about writing machine code instructions, all that was changed was their representation. They were not yet programming languages as we imagine them today.</p>
<figure>
<img src="/ProgrammingTutorial/static/images/edsac_assembly.png" alt="A table explaining EDSAC Assembly as printed in an article in Mathematics of Computation in 1950" /><figcaption>A table explaining EDSAC Assembly as printed in an article in Mathematics of Computation in 1950</figcaption>
</figure>
<p>As computers began to become commercial products, it became clear that assembly language programming was difficult, time consuming, intellectually demanding work and so, like so many other great inventions, programming languages were born from a desire for convenience. Indeed, this was so much the case that, before the term “programming language” originated, the leading term was “simplified coding system.” The notion of a programming language did not really require much of an imaginative leap for the day’s computer engineers and programmers. The academics who formed the early user base of these machines already gave them detailed descriptions of the algorithm the computer had to perform, they just had to deduce how to write this process in terms of the computer’s instructions. The question that led to programming languages was: how do we automate that? You see, as assembly languages had already demonstrated, it was not tremendously difficult to build a program that could take an algorithm in a different notation and turn it into machine code. The obvious dream was for a program that could take a description written in an easy to understand notation and produce ready-to-run machine code, thereby avoiding so much of the time and labour of the assembly programmer. They could go almost straight from human description to machine code. That was the dream.</p>
<p>It is perhaps because this step was so natural to those well placed to take it that we see a fair number of primitive programming languages emerging around the same time (the early 1950s). Indeed, it is impossible to say definitively what was the first programming language in the modern sense. The hard part seemed not to be coming up with a programming language but producing a truly useful one. The leading contender for the title of first was Short Code, proposed by John Mauchly (who was one of the lead designers of ENIAC and its successor machines) in 1949. Short Code was, in effect, a way to include mathematical formulae in an assembly program. However, it had two fatally serious problems: firstly, it required a process of somewhat tedious manual conversions before it was inputted to the computer, and, worse, its designers had chosen to create a program (known as an interpreter) that worked through the code step-by-step converting each part to machine code and running it as it went rather than one (known today as a compiler) that converted all the code to machine instructions before it was run. This meant that every time a Short Code program needed to be used, it was necessary for the computer to carry out the entire conversion process as it ran rather than using its previously generated machine code program but, unfortunately for Short Code, the computers of the day were so slowed down by this interpreting process that is was rendered effectively useless. It wasn’t until a few decades later that, with faster computers, interpreters really took off.</p>
<p>Building a useful compiler, however, proved a far more difficult task. Just like Short Code, many of the earliest have left very little of a mark on history. The compiled language upon which the title of first is most often bestowed is the Autocode developed by Manchester University computer scientist Alick Glennie in 1952. Unlike, Short Code, Autocode programs were less than 10% slower than their machine code equivalents while still being far easier to understand. Autocode was still very different to modern programming languages but it represented, for the first time, how programmers could be relieved from writing machine language instructions. It is unfortunate, then, that (at least, in its initial formulation) very few of the Manchester Mark 1 computer’s programmers adopted it. Two other significant developments in the world of compilers appeared the same year. Firstly, computer programming pioneer Grace Hopper (who we’ll see again shortly) created the first linker: the A-0 System for the UNIVAC I computer. A linker (which was, confusingly, referred to as a compiler at the time) is a key tool used alongside compilers to piece together different parts of machine code into a single program allowing, among other techniques, for programmers to reuse bits of code that solve common tasks. This enabled the creation of extensive libraries of code to help programmers solve different problems. Using libraries remains one of the key aspects of programming today. The other key development of 1952 was the Laning and Zierler system which allowed, for the first time, the use of algebraic-style notation in computer programs. Although the system saw little use at the time, it represented a huge leap in the comprehensibility of programming languages.</p>
<p>By the mid-part of the 1950s, there were programming languages for many of the different computers in operation but they all faced the same problem: each and every model of computer had its own language. These languages were tied deeply to the design of the machine just like machine code was (where each machine had its own set of instructions) and so the programmers had to remain experts in the workings of computers. The dream of languages that followed naturally from human description had not yet been achieved. A major shift in programming language design was needed. This idea was referred to as the machine-independent programming language, a language that could be used for any machine (so long as someone had developed a compiler that could convert it to the relevant machine code) and by programmers without any understanding of the internal technical details. The origin of this idea is often associated with Grace Hopper and, certainly, it is her work as leader of the Eckert–Mauchly Computer Corporation’s automatic programming division that is most responsible for bringing it to fruition.</p>
<figure>
<img src="/ProgrammingTutorial/static/images/hopper.jpg" alt="Grace Hopper (middle), who did much of her computing work in service of the US Navy, is promoted to Rear Admiral on the order of US President Ronald Reagan (left)." /><figcaption>Grace Hopper (middle), who did much of her computing work in service of the US Navy, is promoted to Rear Admiral on the order of US President Ronald Reagan (left).</figcaption>
</figure>
<p>Hopper’s FLOW-MATIC (1955) was the first of the new generation of languages that this shift would bring. Her work with business customers had led her to realise that the symbolic notations that prevailed in existing systems were deeply unpalatable to this group of potential users. Business people had no interest in learning “symbols”, they wanted to describe their processes in something resembling English. Although initially dismissed as unfeasible, FLOW-MATIC was successful and is today recognised as the first English-like programming language. It later formed the basis of the design of the COBOL language (1959) which saw tremendous success with business customers and remains in use in certain places today; it is especially associated with banking systems. Following FLOW-MATIC, two other significant languages emerged which, while not being English-like, represented similarly huge leaps in language design: FORTRAN (1957) and ALGOL (1958). FORTRAN, developed by a team lead by John Backus for IBM, had the most sophisticated compiler of the day: it could apply various optimisations to the code so that it could run faster. It was quickly adopted by scientists and engineers who needed high-performance computing, a use case where it continues to be used to this day. ALGOL, as we’ll see a little later, was perhaps the best designed language of the time: it introduced a plethora of new concepts that made programming easier, more efficient, neater and, in some meaningful way, more elegant than ever before. It became tremendously popular with academic computer scientists and many of the ideas it introduced are at the core of today’s programming languages. Its influence cannot be understated.</p>
<p>What these new languages represented was a fundamental shift in what programming really was and what programmers really did. In the days before, programming had been about learning the workings of a machine and expressing a process in a way that that machine could perform. But machine-independent programming languages had made this so easy, the computer could now do most of that task, and so it was that the job of a programmer changed: it was now about designing a process more than it was about translating it. Programmers were now designers and architects more than they were builders. Perhaps it is this transformation that allowed software to grow in scope to the point that it is now present in so many parts of our lives. <span class="sidenote">A sad consequence of this change in the role of the programmer is that women, who had made up a large proportion of programmers in the early days, were forced out of the industry as programmers came to be seen as more than machine operators and the field gained prestige. Despite recent improvements, computer science is still a long way from recovering the diversity it once had. </span></p>
<p>Through much of the 1960s, the features of programming languages remained relatively unchanged from these ideas. Still, this did not make it a period of contentment: a huge debate was raging. Since the days of raw machine code programming, the flow through programs had been controlled using a technique given the name “Go To” which allowed the program to, depending on certain conditions, go to a different place in its instructions. This tradition was continued in to the first programming languages. ALGOL, however, had introduced some alternative concepts: structures that specified blocks of code to be repeated while a condition was true (iteration) and structures that allowed different code to be run depending on the truth of a condition (selection). ALGOL still included “Go To” but, in most cases, these new structures could obviate its use. A new style of programming was born, one in which use of “Go To” was to be avoided, and was given the moniker of “structured programming.” And so began a fierce debate with advocates of structured programming arguing that “Go To” caused code to be unnecessarily labyrinthine with others vehemently arguing that “Go To” was actually often clearer than the new concepts. A mathematical proof that structured programming could do everything “Go To” could prompted an aggressive rejection of “Go To” by famous computer science Edsger Djikstra in his 1968 letter “Go To Statement Considered Harmful”, to which his opponents quickly published a response in a letter “‘GOTO Considered Harmful’ Considered Harmful” and so it went on. Such was the ferocity of this debate, which continued to consume the programming world well into the 1970s, that Djikstra even wrote that “the use of COBOL [a prominent language without structured programming features] cripples the mind.”</p>
<p>In the end, though, the truth that structured programming <em>did</em> make code easier to read and, in particular, easier to maintain won out against the inertia of the old guard of programmers who believed in “Go To.” Today, structured programming is universally accepted; the battle was won. Towards the end of the 1960s and through much of the 1970s, a huge explosion in programming language diversity occurred, fuelled, perhaps, by the expanding range of use cases for computers. It was in these days that many new styles of programming (known as paradigms, something we’ll look at in more detail in a later section) emerged as well as many of the languages that remain influential today. The language Simula introduced the object-oriented paradigm in 1967 which was developed further by Smalltalk in 1972; also in 1972, Prolog introduced a radical new programming style based in formal logic; in 1973 ML expanded on the functional paradigm that had first been created back in 1958; and, in 1972, perhaps the most significant programming language of all was created, C, which (while not tremendously innovative) became the language of choice for programming language and operating system design — a status which it maintains today — and which continues to be the main source of inspiration for the syntax of many languages. And this is just a small subset of the language developments of the era.</p>
<p>Compared to this veritable explosion of ideas, the 1980s and 1990s were a time of relative restraint with the focus more being on developing and building upon existing ideas rather than creating new ones. With the advent of the home computer, computers and programming ultimately adopted a form (at least, on a technical level) that remains familiar today and it was in this era that many of the most widely used programming languages were first released. Perhaps the most important development, and certainly the most important for us, was the advent of the scripting language. A new style of programming language, for the first time mostly based on interpreters rather than compilers, focussed on quick and easy writing of smaller programs and further removed from hardware details than ever before, these are one of the most influential categories today. Among their number was a language focussed on readability and ease of use (although not especially innovative in its features) and released in 1990 by Dutch programmer Guido van Rossum. This language was Python and, over the more than three decades of its use and development, it has emerged as a highly popular language choice for a wide range of use cases. It is this language that we will use in our exploration of programming. <span class="sidenote"> It’s interesting to note that, to date, no major programming language has ever used diagrammatic notations, despite these being distinctly easier for most people to understand. Is it, as it was with Hopper’s FLOW-MATIC, time for another shift away from computer-centred designs and towards the human?</span></p>

		</div>
		<div class = "controls-container">
			
			<a class="nav-button-edge" href="/ProgrammingTutorial/chapter1/"> ⟵ Chapter 1: An Introduction to Programming</a>
			
			<a class="nav-button-middle" href="/ProgrammingTutorial/">Home</a>
			
			<a class="nav-button-middle" href="/ProgrammingTutorial/chapter1/">Chapter</a>
			
			
		</div>
	</body>
</html>